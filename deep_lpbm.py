# deep_lpbm_minimal.py

import os, json
import glob
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score
import torch
import torch.nn.functional as F
from scipy.optimize import linear_sum_assignment
from sklearn.metrics import confusion_matrix, adjusted_rand_score, normalized_mutual_info_score
from torch_geometric.utils import dense_to_sparse
import seaborn as sns
import networkx as nx
import matplotlib.pyplot as plt
from matplotlib.patches import Wedge, Patch
import matplotlib.colors as mcolors




from class_GCN import *
from class_GCNEncoder import *
# ---------------------------------------------------------------------
# CONFIG
# ---------------------------------------------------------------------
MODE = "disassortative"
DATA_DIR = "data_synthetic/" + MODE# "miscdata" 
RANDOM_STATE = 42
HIDDEN_LAYERS_GCN = 32
MAX_EPOCHS_INIT = 300     # Phase init encodeur (Algorithme 1)
MAX_EPOCHS = 600          # Phase estimation (Algorithme 1)
LEARNING_RATE = 1e-1   # Adam lr=0.01 dans l'article
NUM_SEEDS = 5            # on garde le meilleur ELBO
CLASS_GCN = "GCNEncoder"  # ou "GCN", "GCNEncoder" 


# ---------------------------------------------------------------------
# UTILS
# ---------------------------------------------------------------------

def init_all_params_dur(A, Q, params, eps=1e-5):
    """
    Initialise TOUT pour Deep LPBM :
      - eta0       : N√óQ appartenance douce issue de KMeans (adoucie, stable)
      - z0         : N√ó(Q-1) logits initiaux (inverse softmax centr√© stable)
      - Pi0        : Q√óQ estimation initiale des probabilit√©s de blocs
      - Pi_tilde0  : Q√óQ param√®tre non born√© (nn.Parameter) pour optimiser Œ†

    Tout est cr√©√© directement sur device, et stock√© directement dans params.
    """

    device = torch.device(params.get("device", "cpu"))
    N = A.shape[0]

    A_t = torch.tensor(A, dtype=torch.float32, device=device)

    # ===========================
    # 1) KMEANS ‚Üí labels
    # ===========================
    km = KMeans(n_clusters=Q, n_init="auto", random_state=0).fit(A)
    labels = torch.from_numpy(km.labels_.astype(int)).to(device)

    # ===========================
    # 2) One-hot + adouci (√©vite extr√™mes)
    # ===========================
    c = F.one_hot(labels, num_classes=Q).float().to(device)

    epsi = eps

    # eta0 doux : jamais 1, jamais 0
    eta0 = torch.full_like(c, epsi)
    eta0[c > 0.9] = 1.0 - (Q - 1) * epsi
    eta0 = eta0 / eta0.sum(dim=1, keepdim=True)  # normalisation simplex

    # ===========================
    # 3) z0 = inverse softmax centr√©
    # ===========================
    eta_Q = eta0[:, -1:].clamp_min(epsi)
    z0 = torch.log(eta0[:, :-1].clamp_min(epsi)) - torch.log(eta_Q)

    # stabilisation : pas de z trop grand
    z0 = torch.clamp(z0, -10.0, 10.0)

    # ===========================
    # 4) Estimation stable de Pi0
    # ===========================
    Pi0_num = torch.zeros((Q, Q), device=device)
    Pi0_den = torch.zeros((Q, Q), device=device)
    upper_mask = torch.triu(torch.ones(N, N, device=device), diagonal=1)

    for q in range(Q):
        eta_q = eta0[:, q].unsqueeze(1)   # (N,1)
        for r in range(Q):
            eta_r = eta0[:, r].unsqueeze(0)  # (1,N)
            eta_outer = eta_q @ eta_r        # (N,N)

            num = (A_t * eta_outer * upper_mask).sum()
            den = (eta_outer * upper_mask).sum()

            Pi0_num[q, r] = num
            Pi0_den[q, r] = den + epsi

    Pi0 = Pi0_num / Pi0_den
    Pi0 = 0.5 * (Pi0 + Pi0.T)

    # clamp pour stabilit√©
    Pi0 = Pi0.clamp(0.05, 0.95)

    # ===========================
    # 5) Pi_tilde0 = inverse arctan stable
    # ===========================
    Pi_tilde0 = torch.tan(np.pi * (Pi0 - 0.5))
    Pi_tilde0 = torch.clamp(Pi_tilde0, -10.0, 10.0)

    # IMPORTANT : Pi_tilde0 doit √™tre un vrai nn.Parameter
    Pi_tilde0 = nn.Parameter(Pi_tilde0)

    # ===========================
    # STOCKAGE DANS params
    # ===========================
    params["eta0"]       = eta0
    params["z0"]         = z0
    params["Pi0"]        = Pi0
    params["Pi_tilde0"]  = Pi_tilde0   # directement utilisable dans training

    return eta0, z0, Pi0, Pi_tilde0

def init_all_params(A, Q, params, eps=1e-5, tau=1e-3):
    """
    Initialise TOUT pour Deep LPBM (VERSION DOUCE, type ancien code):

      - eta0       : N√óQ appartenance douce issue de KMeans (one-hot + tau)
      - z0         : N√ó(Q-1) logits initiaux (log((C+tau)/(C_last+tau)))
      - Pi0        : Q√óQ estimation initiale des probabilit√©s de blocs (adoucie)
      - Pi_tilde0  : Q√óQ param√®tre non born√© (nn.Parameter), stabilis√©

    Version empirique STABLE, proche de ton comportement original ‚Üí ARI √©lev√©.
    """

    device = torch.device(params.get("device", "cpu"))
    N = A.shape[0]
    A_t = torch.tensor(A, dtype=torch.float32, device=device)

    # ===========================
    # 1) KMEANS ‚Üí labels
    # ===========================
    km = KMeans(n_clusters=Q, n_init="auto", random_state=0).fit(A)
    labels = torch.from_numpy(km.labels_.astype(int)).to(device)

    # ===========================
    # 2) One-hot dur ‚Üí ADOUCISSEMENT "ancien code"
    # ===========================
    C = F.one_hot(labels, num_classes=Q).float().to(device)  # shape (N,Q)

    # adoucir l√©g√®rement (√©vite extr√™mes, mais garde forme one-hot)
    eta0 = C * (1 - (Q-1)*tau) + (1 - C) * tau
    eta0 = eta0 / eta0.sum(dim=1, keepdim=True)  # normalisation simplex

    # ===========================
    # 3) z0 = LOG-RATIOS (ancienne m√©thode) + stabilisation
    # ===========================
    # z_q = log( (eta_q + tau) / (eta_Q + tau) )
    denom = eta0[:, [Q-1]] + tau
    numer = eta0[:, :Q-1] + tau
    z0 = torch.log(numer) - torch.log(denom)

    # clamp mod√©r√© pour √©viter z explosifs
    z0 = torch.clamp(z0, -8.0, 8.0)

    # ===========================
    # 4) Estimation douce de Pi0 (SBM-like)
    # ===========================
    Pi0_num = torch.zeros((Q, Q), device=device)
    Pi0_den = torch.zeros((Q, Q), device=device)
    upper_mask = torch.triu(torch.ones(N, N, device=device), diagonal=1)

    for q in range(Q):
        eta_q = eta0[:, q].unsqueeze(1)      # (N,1)
        for r in range(Q):
            eta_r = eta0[:, r].unsqueeze(0)  # (1,N)
            eta_outer = eta_q @ eta_r        # (N,N)

            num = (A_t * eta_outer * upper_mask).sum()
            den = (eta_outer * upper_mask).sum()

            Pi0_num[q, r] = num
            Pi0_den[q, r] = den + eps

    Pi0 = Pi0_num / Pi0_den
    Pi0 = 0.5 * (Pi0 + Pi0.T)

    # clamp l√©ger : Pi0 jamais trop extr√™me
    Pi0 = Pi0.clamp(0.05, 0.95)

    # ===========================
    # 5) Pi_tilde0 = inverse arctan (stabilis√©)
    # ===========================
    Pi_tilde0 = torch.tan(np.pi * (Pi0 - 0.5))

    # clamp PiÃÉ0 pour √©viter explosions au tout d√©but
    Pi_tilde0 = torch.clamp(Pi_tilde0, -8.0, 8.0)

    # convertir en nn.Parameter directement ici
    Pi_tilde0 = nn.Parameter(Pi_tilde0)

    # ===========================
    # Stockage dans params
    # ===========================
    params["eta0"]       = eta0
    params["z0"]         = z0
    params["Pi0"]        = Pi0
    params["Pi_tilde0"]  = Pi_tilde0

    return eta0, z0, Pi0, Pi_tilde0


def softmax_logits_to_eta(z):
    """
    - z : N√ó(Q‚àí1) logits per node (last coord has implicit 0-logit)
    Returns Œ∑ (N√óQ) row-stochastic partial memberships; stable softmax via row-wise max.
    Article: Section 2 (logistic-normal with Q‚àí1 logits).
    """
    eps = 1e-12 # ne pas diviser par 0
    N = z.size(0)
    z_pad = torch.cat([z, torch.zeros((N, 1), device=z.device, dtype=z.dtype)], dim=1)
    m = torch.amax(z_pad, dim=1, keepdim=True)                # max ligne
    e = torch.exp(z_pad - m)
    eta = e / (e.sum(dim=1, keepdim=True) + eps)
    return eta

def reparameterize(mu, logvar):
    """
    Reparametrization trick : z = mu + sigma * eps.
    Rappel article : Sec. 4.2 (optimisation stochastique).
    """
    eps = torch.randn_like(mu)
    sigma = torch.exp(0.5 * logvar).expand_as(mu)
    return mu + sigma * eps




def unconstrained_Pi_to_Pi(Pi_tilde):
    """
    f: R -> (0,1) √©l√©ment par √©l√©ment pour obtenir Œ†.
    Rappel article : mapping bijectif pour optimiser Œ†~ sans contraintes (Sec. 4.2).
    f(x) = 0.5 + (1/pi) * arctan(x)
    """
    Pi = 0.5 + torch.atan(Pi_tilde) / np.pi
    return 0.5 * (Pi + Pi.T) # symm√©trie

def init_encoder_phase(A, Q, params, results_dir=None):
    """
    Phase d'initialisation de l‚Äôencodeur GCN (Algorithme 1 / App. B.2).
    - ¬µ ‚âà z‚ÇÄ (issu de KMeans)
    - logœÉ¬≤ ‚âà log(0.01)

    Sauvegarde (dans results_dir) :
        - r√©partition initiale KMeans (CSV + histogramme)
        - courbe de perte d'initialisation init_loss.png
    """
    device   = torch.device(params.get("device", "cpu"))
    hidden   = int(params.get("hidden", HIDDEN_LAYERS_GCN))
    init_lr  = float(params.get("init_lr", LEARNING_RATE))
    seed     = int(params.get("seed", RANDOM_STATE))
    tau      = float(params.get("init_tau", 1e-3))

    torch.manual_seed(seed)
    N = A.shape[0]

    # --- Cr√©ation du dossier (si demand√©) ---
    if results_dir is not None:
        os.makedirs(results_dir, exist_ok=True)

    # --- Mod√®le GCN (cr√©√© une fois et stock√© dans params)
    gcn = GCN(in_feats=N, hidden=hidden, out_mu=Q-1, out_lv=1).to(device)
        

    # --- Optimiseur pour la phase d'init
    opt = torch.optim.Adam(gcn.parameters(), lr=init_lr)


    # --- Tenseurs d‚Äôentr√©e
    A_t = torch.tensor(A, dtype=torch.float32, device=device)
    X   = torch.eye(N, dtype=torch.float32, device=device)

    # --- Initialisation douce via KMeans ---
    eta0, z0, Pi0, Pi_tilde0 = init_all_params(A, Q, params)
    lv_star = torch.full((N, 1), np.log(0.01), dtype=torch.float32, device=device)

    # --- Boucle d‚Äôentra√Ænement (phase d‚Äôinit) ---
    loss_history = []
    gcn.train()
    for _ in range(MAX_EPOCHS_INIT):
        opt.zero_grad()
        mu, logvar = gcn(A_t, X)
        loss_mu = F.mse_loss(mu, z0)
        loss_lv = F.mse_loss(logvar, lv_star)
        loss = loss_mu + loss_lv
        loss.backward()
        opt.step()
        loss_history.append(float(loss.detach().cpu()))

    last_loss = loss_history[-1] if loss_history else None
    params["gcn"] = gcn
    # --- Sauvegarde de la courbe de loss ---
    if results_dir is not None:
        plt.figure()
        plt.plot(loss_history)
        plt.xlabel("It√©rations")
        plt.ylabel("Loss (¬µ, logœÉ¬≤)")
        plt.title(f"Phase d'initialisation GCN (Q={Q})")
        plt.tight_layout()
        plt.savefig(os.path.join(results_dir, "init_loss.png"))
        plt.close()

    return last_loss


def init_encoder_phase_GCNEncoder(A, Q, params, results_dir=None):
    """
    Phase d'initialisation de l‚Äôencodeur GCN (version PyTorch Geometric).

    - ¬µ ‚âà z‚ÇÄ (issu de KMeans sur A)
    - logœÉ¬≤ ‚âà log(0.01)
    - Utilise la nouvelle classe GCN bas√©e sur GCNConv
      (entr√©e : X, edge_index)

    Sauvegarde dans results_dir :
        - r√©partition initiale KMeans (CSV + histogramme)
        - courbe de perte d'initialisation init_loss.png
    """
    device   = torch.device(params.get("device", "cpu"))
    hidden   = int(params.get("hidden", HIDDEN_LAYERS_GCN))
    init_lr  = float(params.get("init_lr", LEARNING_RATE))
    seed     = int(params.get("seed", RANDOM_STATE))
    tau      = float(params.get("init_tau", 1e-3))

    torch.manual_seed(seed)
    N = A.shape[0]

    # --- Cr√©ation du dossier (si demand√©) ---
    if results_dir is not None:
        os.makedirs(results_dir, exist_ok=True)

    # --- Conversion adjacency ‚Üí edge_index ---
    A_t = torch.tensor(A, dtype=torch.float32, device=device)
    edge_index, _ = dense_to_sparse(A_t)

    # --- Features d‚Äôentr√©e : ici identit√©
    X = torch.eye(N, dtype=torch.float32, device=device)

    # --- Mod√®le GCN (cr√©√© une seule fois et stock√© dans params)
    gcn = GCNEncoder(in_feats=N, hidden=hidden, out_mu=Q-1, out_lv=1).to(device)
    

    # --- Optimiseur pour la phase d'init
    opt = torch.optim.Adam(gcn.parameters(), lr=init_lr)
    

    # --- Initialisation douce via KMeans ---
    eta0, z0, Pi0, Pi_tilde0 = init_all_params(A, Q, params)
    # Sauvegarde debug
    save_init_debug_figures(A, eta0, z0, Pi0, Pi_tilde0, results_dir)

    lv_star = torch.full((N, 1), np.log(0.01), dtype=torch.float32, device=device)

    # --- Boucle d‚Äôentra√Ænement (phase d‚Äôinit) ---
    loss_history = []
    gcn.train()
    for _ in range(MAX_EPOCHS_INIT):
        opt.zero_grad()

        # passage GCNConv ‚Üí ¬µ, logœÉ¬≤
        mu, logvar = gcn(X, edge_index)

        # alignement sur z‚ÇÄ et logœÉ¬≤ cible
        loss_mu = F.mse_loss(mu, z0)
        loss_lv = F.mse_loss(logvar, lv_star)
        loss = loss_mu + loss_lv
        loss.backward()
        opt.step()

        loss_history.append(float(loss.detach().cpu()))

    last_loss = loss_history[-1] if loss_history else None

    params["gcn"] = gcn
    # --- Sauvegarde de la courbe de loss ---
    if results_dir is not None:
        plt.figure()
        plt.plot(loss_history)
        plt.xlabel("It√©rations")
        plt.ylabel("Loss (¬µ, logœÉ¬≤)")
        plt.title(f"Phase d'initialisation GCN (Q={Q})")
        plt.tight_layout()
        plt.savefig(os.path.join(results_dir, "init_loss.png"))
        plt.close()

    return last_loss


def decoder_prob(Z, Pi, eps=1e-8):
    """
    - Z  : N√ó(Q‚àí1) logits (√©chantillon)
    - Pi : Q√óQ (matrice de blocs)
    Calcule P (N√óN) avec p_ij = Œ∑_i^T Œ† Œ∑_j (Sec. 2 ‚Üí Œ∑, Sec. 4 ‚Üí d√©codeur bloc).
    """
    # Z ‚Üí Œ∑ (forme Sec. 2 : on ajoute le logit 0 implicite puis softmax stable)
    # NB: softmax_logits_to_eta attend Z de taille N√ó(Q‚àí1) et g√®re le padding interne.
    eta = softmax_logits_to_eta(Z)                # N√óQ
    P = eta @ Pi @ eta.T                          # N√óN
    P = torch.clamp(P, eps, 1 - eps)                  # stabilit√© num.
    return P


# -----------------------------------------------------------
# DEBUG INIT
# -----------------------------------------------------------
def make_debug_dir(results_dir, name="init_debug"):
    debug_dir = os.path.join(results_dir, name)
    os.makedirs(debug_dir, exist_ok=True)
    return debug_dir



def save_init_debug_figures(A, eta0, z0, Pi0, Pi_tilde0, results_dir):

    debug_dir = make_debug_dir(results_dir)

    # --- FIGURE 1 : Heatmap de eta0 ---
    plt.figure(figsize=(8, 6))
    sns.heatmap(eta0.cpu().numpy(), cmap="viridis")
    plt.title("Œ∑‚ÇÄ (initial soft memberships)")
    plt.xlabel("Cluster q")
    plt.ylabel("Node i")
    plt.tight_layout()
    plt.savefig(os.path.join(debug_dir, "eta0_heatmap.png"))
    plt.close()

    # --- FIGURE 2 : Heatmap de z0 ---
    plt.figure(figsize=(8, 6))
    sns.heatmap(z0.cpu().numpy(), cmap="coolwarm", center=0)
    plt.title("z‚ÇÄ (initial logits)")
    plt.xlabel("Dim q")
    plt.ylabel("Node i")
    plt.tight_layout()
    plt.savefig(os.path.join(debug_dir, "z0_heatmap.png"))
    plt.close()

    # --- FIGURE 3 : Histogramme des z0 ---
    plt.figure(figsize=(6, 5))
    plt.hist(z0.cpu().numpy().flatten(), bins=40)
    plt.title("Distribution de z‚ÇÄ")
    plt.tight_layout()
    plt.savefig(os.path.join(debug_dir, "z0_histogram.png"))
    plt.close()

    # --- FIGURE 4 : Heatmap de Pi0 ---
    plt.figure(figsize=(6, 5))
    sns.heatmap(Pi0.cpu().numpy(), annot=True, fmt=".2f", cmap="viridis")
    plt.title("Œ†‚ÇÄ (initial block matrix)")
    plt.xlabel("Cluster r")
    plt.ylabel("Cluster q")
    plt.tight_layout()
    plt.savefig(os.path.join(debug_dir, "Pi0_heatmap.png"))
    plt.close()

    # --- FIGURE 5 : Heatmap de PiÃÉ0 ---
    plt.figure(figsize=(6, 5))
    sns.heatmap(Pi_tilde0.detach().cpu().numpy(), cmap="coolwarm", center=0)
    plt.title("Œ†ÃÉ‚ÇÄ (initial unconstrained block matrix)")
    plt.tight_layout()
    plt.savefig(os.path.join(debug_dir, "Pi_tilde0_heatmap.png"))
    plt.close()

    # --- FIGURE 6 : Histogramme KMeans init ---
    labels = eta0.argmax(dim=1).cpu().numpy()
    plt.figure(figsize=(5, 4))
    plt.hist(labels, bins=np.arange(eta0.shape[1]+1)-0.5, rwidth=0.8)
    plt.xlabel("Cluster")
    plt.ylabel("Count")
    plt.title("Histogramme des clusters KMeans initiaux")
    plt.tight_layout()
    plt.savefig(os.path.join(debug_dir, "kmeans_hist.png"))
    plt.close()

    

# -----------------------------------------------------------
# REPRESENTATION
# -----------------------------------------------------------
def plot_elbo_curve(elbo_history, Q, save_dir):
    """Affiche et enregistre la courbe ELBO."""
    if elbo_history is None or len(elbo_history) == 0:
        return
    plt.figure()
    plt.plot(elbo_history)
    plt.xlabel("It√©rations")
    plt.ylabel("ELBO")
    plt.title(f"√âvolution de l'ELBO (Q={Q})")
    plt.tight_layout()
    os.makedirs(save_dir, exist_ok=True)
    plt.savefig(os.path.join(save_dir, f"ELBO_Q{Q}.png"))
    plt.close()

def plot_Pi_matrix(Pi, Q, save_dir):
    """Affiche et enregistre la matrice Œ† finale."""
    plt.figure()
    plt.imshow(Pi, cmap="viridis")
    plt.colorbar(label="Probabilit√© de connexion")
    plt.title(f"Matrice Œ† finale (Q={Q})")
    plt.xlabel("Cluster j")
    plt.ylabel("Cluster i")
    plt.tight_layout()
    os.makedirs(save_dir, exist_ok=True)
    plt.savefig(os.path.join(save_dir, f"Pi_Q{Q}.png"))
    plt.close()

def plot_eta_histogram(eta, Q, save_dir):
    """Affiche et enregistre l‚Äôhistogramme des clusters (argmax Œ∑)."""
    plt.figure()
    plt.hist(np.argmax(eta, axis=1), bins=np.arange(Q + 1) - 0.5, rwidth=0.8)
    plt.xlabel("Cluster assign√© (argmax Œ∑)")
    plt.ylabel("Nombre de n≈ìuds")
    plt.title(f"Distribution des appartenances (Q={Q})")
    plt.tight_layout()
    os.makedirs(save_dir, exist_ok=True)
    plt.savefig(os.path.join(save_dir, f"eta_hist_Q{Q}.png"))
    plt.close()

def save_all_figures(results_dir, best, Q):
    """Fonction principale pour enregistrer toutes les figures standard."""
    os.makedirs(results_dir, exist_ok=True)
    plot_elbo_curve(best.get("history"), Q, results_dir)
    plot_Pi_matrix(best["Pi"], Q, results_dir)
    plot_eta_histogram(best["eta"], Q, results_dir)


def draw_graph_with_probabilities(A, eta, class_colors=None, class_labels=None, node_radius=None, results_dir=None):
    """
    A : (n,n) 
    eta : (n,k) 
    class_colors : coleurs, facultatives
    node_radius : taille des nodes
    """
    n, k = eta.shape

    if not node_radius:
        node_radius = 1.5/n


    if not class_colors:
        cmap = plt.get_cmap("tab10")
        class_colors = [cmap(i % cmap.N) for i in range(k)]

    
    if class_labels is None:
        class_labels = [f"Class {i}" for i in range(k)]


    print('col', class_colors, 'lab', class_labels)

    G = nx.from_numpy_array(A)
    pos = nx.spring_layout(G, seed=0)

    fig, ax = plt.subplots(figsize=(8,8))
    nx.draw_networkx_edges(G, pos, ax=ax, alpha=0.4)
    
    for i, (x, y) in pos.items():
        start_angle = 90
        for frac, color in zip(eta[i], class_colors):
            if frac <= 0:  # skip empty slices
                continue
            theta1 = start_angle
            theta2 = start_angle + 360 * frac
            wedge = Wedge(center=(x, y),
                          r=node_radius,
                          theta1=theta1,
                          theta2=theta2,
                          facecolor=color,
                          edgecolor='black',
                          linewidth=0.3
                          )
            ax.add_patch(wedge)
            start_angle = theta2
    
    for (x, y) in pos.values():
        circ = plt.Circle((x, y), node_radius, fill=False, edgecolor='black', lw=0.3)
        ax.add_patch(circ)

    legend_patches = [Patch(facecolor=c, edgecolor='black', label=label) for c, label in zip(class_colors, class_labels)]
    ax.legend(handles=legend_patches, loc='upper right', bbox_to_anchor=(1.15, 1))
    

    
    x_values, y_values = zip(*pos.values())
    x_min, x_max = min(x_values), max(x_values)
    y_min, y_max = min(y_values), max(y_values)

    x_range = x_max - x_min
    y_range = y_max - y_min
    margin = 0.2 * max(x_range, y_range)  

    ax.set_xlim(x_min - margin, x_max + margin)
    ax.set_ylim(y_min - margin, y_max + margin)

    ax.set_aspect('equal')
    ax.axis('off')

    ax.relim()        
    ax.autoscale()     

    if results_dir is not None:
        plt.savefig(os.path.join(results_dir, "soft_classification.png"), bbox_inches='tight', dpi=300)
    
    plt.show()
    plt.close()



def draw_graph_hard_clusters(A, y, class_colors=None, class_labels=None, node_size=None, results_dir=None):
    """
    A : (n,n) 
    y : (n,k) 
    class_colors : coleurs, facultatives
    node_size : taille des nodes
    """
    n = len(y)
    k = int(np.max(y))

    if not node_size: 
        node_size = 6000/n

    if not class_colors:
        cmap = plt.get_cmap("tab10")
        class_colors = [mcolors.to_hex(cmap(i % cmap.N)) for i in range(k+1)]
    
    if class_labels is None:
        class_labels = [f"Class {i}" for i in range(k+1)]

    node_colors = [ class_colors[i] for i in y]

    if np.issubdtype(np.array(node_colors).dtype, np.number):
        cmap = plt.get_cmap("tab10")
        node_colors = [cmap(v / max(node_colors)) for v in node_colors]
    

    G = nx.from_numpy_array(A)
    pos = nx.spring_layout(G, seed=0)

    fig, ax = plt.subplots(figsize=(8,8))
    nx.draw_networkx_edges(G, pos, ax=ax, alpha=0.4)
    nx.draw_networkx_nodes(G, pos,
                           node_color=node_colors,
                           node_size=node_size,
                           edgecolors='black',
                           linewidths=0.5,
                           ax=ax)


    for (x, y) in pos.values():
        circ = plt.Circle((x, y), node_size/30000, fill=False, edgecolor='black', lw=0.5)
        ax.add_patch(circ)



    legend_patches = [Patch(facecolor=c, edgecolor='black', label=label) for c, label in zip(class_colors, class_labels)]
    ax.legend(handles=legend_patches, loc='upper right', bbox_to_anchor=(1.15, 1))

    
    ax.set_aspect('equal')
    ax.axis('off')


    if results_dir is not None:
        plt.savefig(os.path.join(results_dir, "hard_clusters.png"), bbox_inches='tight', dpi=300)
    
    plt.show()
    plt.close()




    


# ---------------------------------------------------------------------
# OBJECTIF (ELBO) 
# ---------------------------------------------------------------------


def elbo_stub(A, P, mu, logvar):
    ij = np.triu_indices(A.shape[0], 1)
    ll = (A[ij] * torch.log(P[ij]) + (1-A[ij]) * torch.log(1-P[ij])).sum()

    # Variance isotrope ‚Üí logvar shape (N,1)
    sigma2 = torch.exp(logvar)   # (N,1)
    kl = 0.5 * (sigma2 + mu**2 - 1 - logvar).sum()

    return ll - kl



# ---------------------------------------------------------------------
# ENTRA√éNEMENT (Algorithme 1)
# ---------------------------------------------------------------------

def train_deep_lpbm_GCN(A, Q, seed=RANDOM_STATE, results_dir=None):
    device = torch.device("cpu")
    A_t = torch.tensor(A, dtype=torch.float32, device=device)

    best = {"elbo": -np.inf, "eta": None, "Pi": None}
    for s in range(NUM_SEEDS):
        params = {"Q": Q, "seed": seed + s, "device": device,
                  "hidden": HIDDEN_LAYERS_GCN, "init_lr": LEARNING_RATE}
        
        init_encoder_phase(A, Q, params, results_dir)

        gcn = params["gcn"]
        Pi_tilde = params["Pi_tilde0"]
        #Pi_tilde = torch.zeros((Q, Q), dtype=torch.float32, device=device, requires_grad=True)
        opt = torch.optim.Adam(list(gcn.parameters()) + [Pi_tilde], lr=LEARNING_RATE)

        elbo_history = []
        gcn.train()
        for _ in range(MAX_EPOCHS):
            opt.zero_grad()
            X = torch.eye(A_t.size(0), dtype=torch.float32, device=device)
            mu, logvar = gcn(A_t, X)
            z = reparameterize(mu, logvar)
            Pi = unconstrained_Pi_to_Pi(Pi_tilde)
            P = decoder_prob(z, Pi)
            elbo = elbo_stub(A_t, P, mu, logvar)

            loss = -elbo
            loss.backward()
            opt.step()
            elbo_history.append(float(elbo.detach().cpu()))

        # √âvaluation finale
        gcn.eval()
        with torch.no_grad():
            X = torch.eye(A_t.size(0), dtype=torch.float32, device=device)
            mu, logvar = gcn(A_t, X)
            z = reparameterize(mu, logvar)
            eta = softmax_logits_to_eta(z).cpu().numpy()
            Pi = unconstrained_Pi_to_Pi(Pi_tilde).cpu().numpy()

        last_elbo = elbo_history[-1]
        if last_elbo > best["elbo"]:
            best.update({"elbo": last_elbo, "eta": eta, "Pi": Pi, "history": elbo_history})

    # Sauvegarde des figures (si demand√©)
    if results_dir is not None:
        save_all_figures(results_dir, best, Q)

    return best



def train_deep_lpbm_GCNEncoder(A, Q, seed=RANDOM_STATE, results_dir=None):
    """
    Entra√Æne un mod√®le Deep LPBM avec encodeur GCN probabiliste (version PyG).
    - Utilise la nouvelle classe GCN bas√©e sur GCNConv (entr√©e sparse edge_index).
    """
    device = torch.device("cpu")
    torch.manual_seed(seed)

    # Convertit la matrice dense A en format edge_index pour PyG
    A_t = torch.tensor(A, dtype=torch.float32, device=device)
    edge_index, _ = dense_to_sparse(A_t)     # (2, E)
    N = A_t.size(0)

    # Features : vecteur identit√© ou constantes (selon ton choix)
    X = torch.eye(N, dtype=torch.float32, device=device)

    best = {"elbo": -np.inf, "eta": None, "Pi": None}

    for s in range(NUM_SEEDS):
        torch.manual_seed(seed + s)

        # Initialisation du mod√®le GCN (d√©fini ailleurs avec GCNConv)
        params = {"Q": Q, "seed": seed + s, "device": device,
                  "hidden": HIDDEN_LAYERS_GCN, "init_lr": LEARNING_RATE}
        init_encoder_phase_GCNEncoder(A, Q, params, results_dir)
        gcn = params["gcn"]  # ta classe GCN (avec GCNConv)

        # Param√®tres du LPBM
        Pi_tilde = params["Pi_tilde0"]
        #Pi_tilde = torch.zeros((Q, Q), dtype=torch.float32, device=device, requires_grad=True)
        opt = torch.optim.Adam(list(gcn.parameters()) + [Pi_tilde], lr=LEARNING_RATE)

        elbo_history = []

        # === Phase d'entra√Ænement ===
        gcn.train()
        for _ in range(MAX_EPOCHS):
            opt.zero_grad()

            # Passage GCN ‚Üí ¬µ et logœÉ¬≤
            mu, logvar = gcn(X, edge_index)

            # R√©√©chantillonnage et calcul ELBO
            z = reparameterize(mu, logvar)
            Pi = unconstrained_Pi_to_Pi(Pi_tilde)
            P = decoder_prob(z, Pi)
            elbo = elbo_stub(A_t, P, mu, logvar)

            # Backpropagation
            loss = -elbo
            loss.backward()
            opt.step()

            elbo_history.append(float(elbo.detach().cpu()))

        # === Phase d'√©valuation ===
        gcn.eval()
        with torch.no_grad():
            mu, logvar = gcn(X, edge_index)
            z = reparameterize(mu, logvar)
            eta = softmax_logits_to_eta(z).cpu().numpy()
            Pi = unconstrained_Pi_to_Pi(Pi_tilde).cpu().numpy()

        last_elbo = elbo_history[-1]
        if last_elbo > best["elbo"]:
            best.update({
                "elbo": last_elbo,
                "eta": eta,
                "Pi": Pi,
                "history": elbo_history
            })

    # === Sauvegarde des figures ===
    if results_dir is not None:
        save_all_figures(results_dir, best, Q)

    print(f"[Deep LPBM + GCNConv] Q={Q} ‚Äî ELBO finale : {best['elbo']:.4f}")
    return best

# ---------------------------------------------------------------------
# S√âLECTION DE MOD√àLE (Q) ‚Äî crit√®res info
# ---------------------------------------------------------------------
def loglikelihood_A_given_eta_Pi(A, eta, Pi):
    """
    ln p(A | Œ∑, Œ†) (Bernoulli ind√©pendante par ar√™te, non orient√©).
    Rappel article : Sec. 4.3 (crit√®res info).
    """
    eps = 1e-12
    # p_ij = Œ∑_i^T Œ† Œ∑_j
    P = eta @ Pi @ eta.T
    P = np.clip(P, eps, 1 - eps)  # stabilit√© num. pour log
    # somme sur i<j (graphe non orient√©, pas de boucles)
    ij = np.triu_indices_from(A, k=1)
    ll = (A[ij] * np.log(P[ij]) + (1 - A[ij]) * np.log(1 - P[ij])).sum()
    return float(ll)



def compute_AIC_BIC_ICL(A, eta, Pi):
    """
    AIC, BIC, ICL (avec Œ∑ fix√©, comme dans l‚Äôarticle).
    Rappel article : Sec. 4.3 (+ r√©sultats Sec. 5.2).
    """
    N = A.shape[0]
    Q = eta.shape[1]
    ll_A = loglikelihood_A_given_eta_Pi(A, eta, Pi)
    nu_NQ = Q*(Q+1) + N*(Q-1)
    nu_NQ_Pi = 0.5*Q*(Q+1)
    Nobs = 0.5*N*(N-1)

    AIC = ll_A - nu_NQ
    BIC = ll_A - 0.5*nu_NQ*np.log(Nobs)
    # ICL approx (remplacer par ln p(A,Z|Œ∏) si tu utilises une partition dure)
    ICL = ll_A - 0.5*nu_NQ_Pi*np.log(Nobs)
    return AIC, BIC, ICL

def collapse_small_clusters(eta, min_size_ratio=0.03):
    """
    Prend une matrice d'appartenance partielle Œ∑ (N√óQ)
    ‚Üí d√©tecte les clusters trop petits (<3%)
    ‚Üí redistribue leurs n≈ìuds vers le cluster suivant le plus probable
    ‚Üí retourne une nouvelle Œ∑_collapsed (m√™me taille N√óQ)
    """
    N, Q = eta.shape
    min_size = max(1, int(min_size_ratio * N))

    # assignation dure
    y = eta.argmax(axis=1)
    
    # taille des clusters
    counts = np.bincount(y, minlength=Q)
    small = np.where(counts < min_size)[0]

    if len(small) == 0:
        return eta  # rien √† faire

    eta_new = eta.copy()

    for sc in small:
        nodes = np.where(y == sc)[0]
        if len(nodes) == 0:
            continue

        # pour ces noeuds, prendre la 2e proba la plus forte
        for i in nodes:
            # trier par probas d√©croissantes
            order = np.argsort(eta[i])[::-1]
            # prendre la plus grande ‚â† sc
            for alt in order:
                if alt != sc:
                    eta_new[i, sc] = 0.0
                    eta_new[i, alt] = 1.0
                    break

    # renormalisation simple (pas toujours n√©cessaire car one-hot)
    eta_new = eta_new / eta_new.sum(axis=1, keepdims=True)
    print("collapsed!")
    return eta_new


def model_selection_over_Q(A, Q_list, subject_name="subject", seed=RANDOM_STATE, CLASS_GCN = CLASS_GCN):
    """
    Essaie plusieurs valeurs de Q et renvoie le meilleur mod√®le selon l'AIC.
    Cr√©e un sous-dossier results/<subject_name>/Q_<Q>/ pour chaque entra√Ænement.
    """
    results_dir_base = os.path.join("results", subject_name)
    os.makedirs(results_dir_base, exist_ok=True)

    results = []
    AIC_log = []
    for Q in Q_list:
        print(f"\n=== Entra√Ænement pour Q = {Q} ===")
        results_dir_Q = os.path.join(results_dir_base, f"Q_{Q}")
        if CLASS_GCN == "GCN":
            fit = train_deep_lpbm_GCN(A, Q, seed=seed, results_dir=results_dir_Q)
        if CLASS_GCN == "GCNEncoder":
            fit = train_deep_lpbm_GCNEncoder(A, Q, seed=seed, results_dir=results_dir_Q)
        
        # collapse √©ventuel des petits clusters
        eta_collapsed = collapse_small_clusters(fit["eta"], min_size_ratio=0.03)
        fit["eta"] = eta_collapsed

        # recalcul des crit√®res d'information
        AIC, BIC, ICL = compute_AIC_BIC_ICL(A, fit["eta"], fit["Pi"])
        AIC_log.append(AIC)
        fit.update({"Q": Q, "AIC": AIC, "BIC": BIC, "ICL": ICL})
        results.append(fit)

        #draw_graph_with_probabilities(A, fit["eta"], results_dir=results_dir_Q)

        y_hat = fit["eta"].argmax(axis=1)
        #draw_graph_hard_clusters(A, y_hat, results_dir=results_dir_Q)


        # Sauvegarde rapide des scores num√©riques
        with open(os.path.join(results_dir_Q, "scores.txt"), "w") as f:
            f.write(f"AIC: {AIC:.3f}\nBIC: {BIC:.3f}\nICL: {ICL:.3f}\nELBO: {fit['elbo']:.3f}\n")

    # S√©lection du meilleur mod√®le selon AIC (comme recommand√© dans l‚Äôarticle)
    best = max(results, key=lambda d: d["AIC"])
    best_Q = best["Q"]

    plt.figure(figsize=(6, 4))

    plt.plot(Q_list, AIC_log, marker='o')
    plt.vlines(best_Q, ymin=min(AIC_log), ymax=max(AIC_log),
            color='red', linestyle='--', label='Best Q')

    plt.title("AIC values for each tested Q")
    plt.xlabel("Number of clusters Q")
    plt.ylabel("AIC")
    plt.legend()
    plt.tight_layout()

    os.makedirs(results_dir_base, exist_ok=True)
    plt.savefig(os.path.join(results_dir_base, "AIC_over_Q.png"))
    plt.close()


    return best, results


# ---------------------------------------------------------------------
# √âVALUATION
# ---------------------------------------------------------------------
def hard_clusters_from_eta(eta):
    """
    Partition dure par argmax(Œ∑) (utile pour ARI).
    Rappel article : m√©triques (Sec. 5 / App. C.1).
    """
    return eta.argmax(axis=1)

def H_partial_memberships_score(eta_true, eta_hat):
    """
    M√©trique H (Sec. 5) : distance L1 normalis√©e entre co-appartenances U*=Œ∑*Œ∑*^T et √õ=Œ∑ÃÇŒ∑ÃÇ^T,
    somm√©e sur i<j (paires uniques), invariante aux permutations de clusters.
    """
    eps = 1e-12
    
    U_true = eta_true @ eta_true.T
    U_hat  = eta_hat  @ eta_hat.T
    N = eta_true.shape[0]
    tri = np.triu_indices(N, k=0)  # i<j, exclut la diagonale
    diff = np.abs(U_true - U_hat)[tri]
    return np.sqrt(2.0/(N*(N-1))) * diff.sum()



def cluster_counts(labels):
    """Renvoie un dictionnaire {cluster_id: count}."""
    uniq, counts = np.unique(labels, return_counts=True)
    return dict(zip(uniq, counts))


def best_label_permutation(y_true, y_pred):
    """
    Trouve la meilleure permutation des labels pr√©dits pour matcher y_true.
    """
    classes_true = np.unique(y_true)
    classes_pred = np.unique(y_pred)
    n_true, n_pred = len(classes_true), len(classes_pred)
    cost_matrix = np.zeros((n_true, n_pred))
    for i, c_true in enumerate(classes_true):
        for j, c_pred in enumerate(classes_pred):
            # co√ªt = nombre d‚Äôerreurs (on veut le minimiser)
            cost_matrix[i, j] = np.sum((y_true == c_true) & (y_pred != c_pred))
    row_ind, col_ind = linear_sum_assignment(cost_matrix)
    mapping = {classes_pred[j]: classes_true[i] for i, j in zip(row_ind, col_ind)}
    return mapping

def relabel_predictions(y_pred, mapping):
    """Applique la correspondance optimale (mapping) aux labels pr√©dits."""
    y_aligned = np.copy(y_pred)
    for old, new in mapping.items():
        y_aligned[y_pred == old] = new
    return y_aligned



def confusion_matrix_permuted(y_true, y_pred):
    mapping = best_label_permutation(y_true, y_pred)
    y_aligned = relabel_predictions(y_pred, mapping)
    cm = confusion_matrix(y_true, y_aligned)
    return cm, mapping, y_aligned



def show_confusion_matrix(y_true, y_pred):
    cm, mapping = confusion_matrix_permuted(y_true, y_pred)
    df = pd.DataFrame(
        cm,
        index=[f"True {c}" for c in np.unique(y_true)],
        columns=[f"Pred {c}" for c in np.unique(y_true)]
    )
    print("üîπ Correspondance optimale :", mapping)
    print("\n=== Matrice de confusion (apr√®s permutation) ===")
    print(df)

def global_clustering_scores(y_true, y_pred):
    """
    Calcule ARI et NMI (robustes aux permutations de labels).
    """
    ari = adjusted_rand_score(y_true, y_pred)
    nmi = normalized_mutual_info_score(y_true, y_pred)
    print(f"ARI : {ari:.3f}")
    print(f"NMI : {nmi:.3f}")
    return ari, nmi


def evaluate_clustering(y_true=None, y_pred=None,
                        eta_true=None, eta_hat=None,
                        Pi_true=None, Pi_pred=None):
    """
    √âvalue un clustering dur ou partiel + compare Pi_true / Pi_pred si fournis.
    """
    print("=== √âvaluation du mod√®le ===")

    # ---- Comparaison des matrices Œ† si pr√©sentes ----
    if (Pi_true is not None) and (Pi_true.shape == Pi_pred.shape) :
        diff = np.abs(Pi_true - Pi_pred).mean()
        print(f"\n=== Comparaison Œ† ===")
        print(f"Diff√©rence moyenne |Œ†_true - Œ†_pred| = {diff:.4f}")
        print(f"Shape Œ†_true = {Pi_true.shape} | Shape Œ†_pred = {Pi_pred.shape}")

    # ---- Cas des appartenances partielles ----
    if eta_true is not None and eta_hat is not None:
        H = H_partial_memberships_score(eta_true, eta_hat)
        y_true_hard = hard_clusters_from_eta(eta_true)
        y_pred_hard = hard_clusters_from_eta(eta_hat)
        print(f"\nScore H (appartenances partielles) = {H:.4f}")
        y_true, y_pred = y_true_hard, y_pred_hard

    # ---- Cas des labels durs ----
    if y_true is not None and y_pred is not None:
        cm, mapping, y_aligned = confusion_matrix_permuted(y_true, y_pred)

        labels = sorted(set(y_true) | set(y_pred))

        df = pd.DataFrame(
            cm,
            index=[f"True {c}" for c in labels],
            columns=[f"Pred {c}" for c in labels]
        )


        ari = adjusted_rand_score(y_true, y_pred)
        nmi = normalized_mutual_info_score(y_true, y_pred)

        print("\n=== Matrice de confusion ===")
        print(df)

        print("\n=== Scores globaux ===")
        print(f"ARI = {ari:.3f}")
        print(f"NMI = {nmi:.3f}")

        for c in np.unique(y_true):
            mask = y_true == c
            correct = np.sum(y_aligned[mask] == c)
            total = np.sum(mask)
            print(f"Cluster {c}: {correct}/{total} corrects ({100*correct/total:.1f}%)")

    elif eta_true is None or eta_hat is None:
        print(" Fournis au moins y_true/y_pred ou eta_true/eta_hat.")


def save_node_assignments(y_true, y_pred, mapping, subject_name,  results_root="results"):
    """
    Enregistre les assignations vraies/pr√©dites des noeuds dans le dossier results/<patient>/assignments/.
    Le nom du fichier correspond √† la matrice observ√©e (ex: results/subject01/assignments/subject01.json).
    """
    # --- Pr√©paration du dossier ---
    out_dir = os.path.join(results_root, subject_name, "assignments")
    os.makedirs(out_dir, exist_ok=True)

    # --- Applique la permutation optimale aux pr√©dictions ---
    y_aligned = relabel_predictions(y_pred, mapping)

    # --- Construit un tableau clair ---
    data = {
        "node_id": np.arange(len(y_true)).tolist(),
        "true_label": y_true.astype(int).tolist(),
        "pred_label_raw": y_pred.astype(int).tolist(),
        "pred_label_aligned": y_aligned.astype(int).tolist()
    }
    df = pd.DataFrame(data)

    # --- Noms des fichiers ---
    json_path = os.path.join(out_dir, f"{subject_name}.json")
    csv_path  = os.path.join(out_dir, f"{subject_name}.csv")

    # --- Sauvegarde ---
    with open(json_path, "w") as f:
        json.dump(data, f, indent=2)
    df.to_csv(csv_path, index=False)

    print(f"\nAssignations enregistr√©es dans :\n  {json_path}\n  {csv_path}")

# ---------------------------------------------------------------------
# PIPELINE EXEMPLE (consomme tes .npy)
# ---------------------------------------------------------------------

def save_Pi_comparison(Pi_true, Pi_pred, outpath):
    """
    Cr√©e et sauvegarde une figure comparant Œ†_true et Œ†_pred.
    """
    fig, axes = plt.subplots(1, 2, figsize=(10, 4))

    im0 = axes[0].imshow(Pi_true, cmap="viridis")
    axes[0].set_title("Œ† vraie (ground truth)")
    plt.colorbar(im0, ax=axes[0], fraction=0.046)

    im1 = axes[1].imshow(Pi_pred, cmap="viridis")
    axes[1].set_title("Œ† pr√©dite (Deep LPBM)")
    plt.colorbar(im1, ax=axes[1], fraction=0.046)

    plt.suptitle("Comparaison des matrices Œ†", fontsize=14)
    plt.tight_layout()
    plt.savefig(outpath, dpi=200)
    plt.close()


def main():
    # --- 2. S√©lection du sujet ---
    SUBJECT_IDX = 3
    A_filename = f"A_{SUBJECT_IDX:03d}.npy"     
    A_path = os.path.join(DATA_DIR, A_filename) 

    A = np.load(A_path)
    
    subject_name = os.path.splitext(os.path.basename(A_path))[0].replace("A_", "")
    print(f"Sujet s√©lectionn√© : {subject_name} dans le dossier {DATA_DIR}")

    # --- 3. S√©lection du nombre de clusters ---
    Q_list = [6,5,4,3]
    best, all_results = model_selection_over_Q(A, Q_list, subject_name=MODE + subject_name)

    # --- 4. R√©sum√© du meilleur mod√®le ---
    print("\n=== Meilleur mod√®le ===")
    print(f"Q optimal (AIC) : {best['Q']}")
    print(f"AIC : {best['AIC']:.2f}   |   BIC : {best['BIC']:.2f}   |   ICL : {best['ICL']:.2f}")
    print(f"Œ∑ shape : {best['eta'].shape}   |   Œ† shape : {best['Pi'].shape}")


    # --- 5. Partition dure (clusters pr√©dits) ---
    y_hat = hard_clusters_from_eta(best["eta"])
    Pi = best['Pi']

    # --- 6. Recherche d‚Äôun fichier y_* pour v√©rifier s‚Äôil s‚Äôagit de donn√©es synth√©tiques ---
    y_path = os.path.join(DATA_DIR, os.path.basename(A_path).replace("A_", "y_"))
    Pi_path = os.path.join(DATA_DIR, os.path.basename(A_path).replace("A_", "Pi_"))
    eta_path = os.path.join(DATA_DIR, os.path.basename(A_path).replace("A_", "eta_"))
    
    if os.path.exists(y_path):
        print("\n=== Donn√©es synth√©tiques d√©tect√©es ===")
        y = np.load(y_path).astype(int).ravel()
        K_true = np.unique(y).size
        print(f"K vrai : {K_true}   |   Q trouv√© : {best['Q']}")

        # --- 7. Cr√©ation de Œ∑* (one-hot de y vrai) ---
        eta_true = np.load(eta_path)
        Pi_true = np.load(Pi_path)

        # --- 8. √âvaluation ---
        evaluate_clustering(
            y_true=y,
            y_pred=y_hat,
            eta_true=eta_true,
            eta_hat=best["eta"],
            Pi_true=Pi_true,
            Pi_pred=Pi
        )

        # --- 9. Sauvegarde des assignations ---
        save_node_assignments(
            y_true=y,
            y_pred=y_hat,
            mapping=best_label_permutation(y, y_hat),
            subject_name=MODE + subject_name,
            results_root="results"
        )

        # --- 10. Sauvegarde de Œ† vraie et pr√©dite ---
    
        results_dir = os.path.join("results", MODE + subject_name)
        os.makedirs(results_dir, exist_ok=True)

        fig_path = os.path.join(results_dir, "Pi_comparison.png")
        save_Pi_comparison(Pi_true, Pi, fig_path)
    else:
        # --- Cas r√©el : pas de v√©rit√© terrain ---
        print("\n=== Donn√©es r√©elles ===")
        print("Aucune v√©rit√© terrain disponible.")
        print("Comptages par cluster pr√©dits :", cluster_counts(y_hat))


if __name__ == "__main__":
    main()
